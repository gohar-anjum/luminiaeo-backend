#!/usr/bin/env python3
"""
Complete Training Script for Custom Keyword Clustering Model

This script trains a custom sentence transformer model optimized for keyword clustering.
Use this after preparing training pairs with prepare_dataset_for_training.py
"""

import json
import os
import sys
import argparse
from pathlib import Path
from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import logging
from datetime import datetime

os.environ['PYTHONUNBUFFERED'] = '1'
sys.stdout.reconfigure(line_buffering=True) if hasattr(sys.stdout, 'reconfigure') else None

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    stream=sys.stdout,
    force=True
)
logger = logging.getLogger(__name__)


def load_training_pairs(file_path: str):
    """Load training pairs from JSON file"""
    logger.info(f"Loading training pairs from {file_path}...")
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if 'training_pairs' in data:
        pairs = data['training_pairs']
        logger.info(f"Loaded {len(pairs):,} training pairs")
        logger.info(f"  Positive: {data.get('positive_count', 0):,}")
        logger.info(f"  Negative: {data.get('negative_count', 0):,}")
        return pairs
    return data


def prepare_examples(pairs):
    """Convert training pairs to InputExample format"""
    logger.info("Converting pairs to InputExample format...")
    examples = []
    for pair in pairs:
        if len(pair) == 3:
            keyword1, keyword2, similarity = pair
            examples.append(InputExample(texts=[keyword1, keyword2], label=float(similarity)))
        else:
            logger.warning(f"Invalid pair format: {pair}")
    logger.info(f"Prepared {len(examples):,} examples")
    return examples


def train_model(
    training_pairs_file: str,
    output_dir: str = './models/custom-keyword-clustering',
    base_model: str = 'sentence-transformers/all-mpnet-base-v2',
    train_ratio: float = 0.8,
    epochs: int = 3,
    batch_size: int = 16,
    learning_rate: float = 2e-5,
    warmup_steps: int = 100,
    evaluation_steps: int = 500,
    resume: bool = False,
):
    """
    Train a custom sentence transformer model for keyword clustering
    
    Args:
        training_pairs_file: Path to training pairs JSON file
        output_dir: Directory to save the trained model
        base_model: Base model to fine-tune
        train_ratio: Ratio of data for training (rest for validation)
        epochs: Number of training epochs
        batch_size: Training batch size
        learning_rate: Learning rate
        warmup_steps: Number of warmup steps
        evaluation_steps: Steps between evaluations
    """
    start_time = datetime.now()
    logger.info("=" * 60)
    logger.info("Starting Custom Keyword Clustering Model Training")
    logger.info("=" * 60)
    
    # Load training pairs
    pairs = load_training_pairs(training_pairs_file)
    
    if not pairs:
        raise ValueError("No training pairs found in file")
    
    # Convert to examples
    examples = prepare_examples(pairs)
    
    # Split into train and validation
    logger.info(f"Splitting data (train: {train_ratio*100:.1f}%, validation: {(1-train_ratio)*100:.1f}%)...")
    train_examples, val_examples = train_test_split(
        examples,
        train_size=train_ratio,
        random_state=42,
        shuffle=True
    )
    logger.info(f"Train examples: {len(train_examples):,}")
    logger.info(f"Validation examples: {len(val_examples):,}")
    
    # Load model (resume from checkpoint or load base model)
    if resume and os.path.exists(output_dir) and os.path.isdir(output_dir):
        if os.path.exists(os.path.join(output_dir, 'model.safetensors')) or os.path.exists(os.path.join(output_dir, 'pytorch_model.bin')):
            logger.info(f"Resuming training from checkpoint: {output_dir}")
            logger.info("Loading partially trained model...")
            model = SentenceTransformer(output_dir)
            logger.info("Partially trained model loaded successfully")
            
            # Check evaluation results to see progress
            eval_file = os.path.join(output_dir, 'eval', 'similarity_evaluation_validation_results.csv')
            if os.path.exists(eval_file):
                import csv
                with open(eval_file, 'r') as f:
                    reader = csv.DictReader(f)
                    evaluations = list(reader)
                    if evaluations:
                        last_eval = evaluations[-1]
                        logger.info(f"Last evaluation: Epoch {last_eval.get('epoch', '?')}, Step {last_eval.get('steps', '?')}")
        else:
            logger.info(f"No checkpoint found at {output_dir}, starting fresh training...")
            logger.info(f"Loading base model: {base_model}...")
            model = SentenceTransformer(base_model)
            logger.info("Base model loaded successfully")
    else:
        logger.info(f"Loading base model: {base_model}...")
        logger.info("This may take a moment (model will be downloaded if not cached)...")
        model = SentenceTransformer(base_model)
        logger.info("Base model loaded successfully")
    
    # Prepare data loaders
    logger.info(f"Preparing data loaders (batch size: {batch_size})...")
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
    
    # Define loss function
    logger.info("Initializing loss function (CosineSimilarityLoss)...")
    train_loss = losses.CosineSimilarityLoss(model)
    
    # Prepare evaluator
    logger.info("Preparing validation evaluator...")
    evaluator = EmbeddingSimilarityEvaluator.from_input_examples(
        val_examples,
        name='validation'
    )
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Output directory: {output_dir}")
    
    # Training configuration
    logger.info("=" * 60)
    logger.info("Training Configuration:")
    logger.info(f"  Base model: {base_model}")
    logger.info(f"  Epochs: {epochs}")
    logger.info(f"  Batch size: {batch_size}")
    logger.info(f"  Learning rate: {learning_rate}")
    logger.info(f"  Warmup steps: {warmup_steps}")
    logger.info(f"  Evaluation steps: {evaluation_steps}")
    logger.info(f"  Train examples: {len(train_examples):,}")
    logger.info(f"  Validation examples: {len(val_examples):,}")
    logger.info("=" * 60)
    
    # Train the model
    logger.info("Starting training...")
    logger.info("This may take several hours depending on dataset size and hardware...")
    
    total_steps = len(train_dataloader) * epochs
    steps_per_epoch = len(train_dataloader)
    logger.info(f"Total training steps: {total_steps:,} ({steps_per_epoch:,} steps per epoch × {epochs} epochs)")
    
    try:
        import sys
        is_tty = sys.stdout.isatty()
        
        logger.info("Training started - progress will be logged every evaluation step")
        
        model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            evaluator=evaluator,
            epochs=epochs,
            evaluation_steps=evaluation_steps,
            warmup_steps=warmup_steps,
            output_path=output_dir,
            optimizer_params={'lr': learning_rate},
            show_progress_bar=is_tty,
        )
        
        logger.info("Training epoch completed - logging progress...")
        
        logger.info("=" * 60)
        logger.info(f"✅ Training complete! Model saved to {output_dir}")
        logger.info("=" * 60)
        
        # Save training metadata
        metadata = {
            'base_model': base_model,
            'training_pairs_count': len(pairs),
            'train_examples': len(train_examples),
            'val_examples': len(val_examples),
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'train_ratio': train_ratio,
            'warmup_steps': warmup_steps,
            'training_started': start_time.isoformat(),
            'training_completed': datetime.now().isoformat(),
            'training_duration_seconds': (datetime.now() - start_time).total_seconds(),
        }
        
        metadata_path = os.path.join(output_dir, 'training_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"Training metadata saved to {metadata_path}")
        
        # Calculate duration
        duration = datetime.now() - start_time
        logger.info(f"Total training time: {duration}")
        
        return True
        
    except Exception as e:
        logger.error(f"Training failed: {e}", exc_info=True)
        raise


def main():
    parser = argparse.ArgumentParser(
        description='Train custom keyword clustering model',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        'training_pairs',
        help='Path to training pairs JSON file',
        default='training_pairs.json',
        nargs='?'
    )
    parser.add_argument(
        '--output-dir',
        default='./models/custom-keyword-clustering',
        help='Directory to save trained model'
    )
    parser.add_argument(
        '--base-model',
        default='sentence-transformers/all-mpnet-base-v2',
        help='Base model to fine-tune'
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=3,
        help='Number of training epochs'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=16,
        help='Training batch size'
    )
    parser.add_argument(
        '--learning-rate',
        type=float,
        default=2e-5,
        help='Learning rate'
    )
    parser.add_argument(
        '--train-ratio',
        type=float,
        default=0.8,
        help='Ratio of data for training (0-1)'
    )
    parser.add_argument(
        '--warmup-steps',
        type=int,
        default=100,
        help='Number of warmup steps'
    )
    parser.add_argument(
        '--evaluation-steps',
        type=int,
        default=500,
        help='Steps between evaluations'
    )
    parser.add_argument(
        '--resume',
        action='store_true',
        help='Resume training from checkpoint in output directory'
    )
    
    args = parser.parse_args()
    
    # Check if training pairs file exists
    if not os.path.exists(args.training_pairs):
        logger.error(f"Training pairs file not found: {args.training_pairs}")
        logger.info("Please prepare training data first using:")
        logger.info("  python prepare_dataset_for_training.py ../storage/app/dataset_10000000_pairs.json --output training_pairs.json")
        return 1
    
    try:
        train_model(
            training_pairs_file=args.training_pairs,
            output_dir=args.output_dir,
            base_model=args.base_model,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            train_ratio=args.train_ratio,
            warmup_steps=args.warmup_steps,
            evaluation_steps=args.evaluation_steps,
            resume=args.resume,
        )
        return 0
    except Exception as e:
        logger.error(f"Training failed: {e}")
        return 1


if __name__ == '__main__':
    exit(main())


